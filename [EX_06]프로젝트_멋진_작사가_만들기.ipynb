{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1F0SZnnHxvYtWfqFiLIDNBTqxwleQatfW",
      "authorship_tag": "ABX9TyMb9AIuhF3QAY5i11SyJcIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonu96/first-repository/blob/main/%5BEX_06%5D%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_%EB%A9%8B%EC%A7%84_%EC%9E%91%EC%82%AC%EA%B0%80_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 멋진 작사가 만들기\n",
        "라이브러리 버전을 확인해 봅니다"
      ],
      "metadata": {
        "id": "pXgzpWgei86V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8poR54miF1Kn",
        "outputId": "b316c0ab-f87f-411d-da24-573ec1937ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. 데이터 다운로드\n",
        "\n",
        "노드에 있는 쥬피터에서 파일을 꺼내 구글 드라이브에 업로드.\n"
      ],
      "metadata": {
        "id": "g3-BWWIxjHLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. 데이터 읽어오기"
      ],
      "metadata": {
        "id": "w1kZlgL_jt_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/aiffel/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
        "\n",
        "raw_corpus = [] \n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtlQc9pWMWjK",
        "outputId": "b28aef53-a0ee-4bc2-fb65-ec4f63c14155"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. 데이터 정제"
      ],
      "metadata": {
        "id": "UBH10-ETj0c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "    if sentence[-1] == \":\": continue  # 문장의 끝이 ) 인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
        "        \n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qf3y1WLbV0W",
        "outputId": "aa4bc53a-3680-4942-d8ac-68765350df10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
        "  \n",
        "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "M9BgiUcRbaoK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "특수문자 등을 바꿔주는 작업을 한다."
      ],
      "metadata": {
        "id": "Hg-gt1Nq2S_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "        \n",
        "    corpus.append(preprocess_sentence(sentence))\n",
        "        \n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9WGFa-Ibe9k",
        "outputId": "bf28f3cf-deff-425b-e0f3-bea5d52e40c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. 평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "ARWpFU1Hkh4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000,  # 전체 단어의 개수 \n",
        "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
        "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
        "\n",
        "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
        "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "05GjGFbagVp8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "토크나이저 생성"
      ],
      "metadata": {
        "id": "dQuWB_fh2kNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uCbLlSLgYyF",
        "outputId": "f40bb2f7-0d10-41dc-ed28-b532e49c2555"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 304  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2  23  77 ...   0   0   0]\n",
            " [  2  42  26 ...   0   0   0]\n",
            " [  2  23  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f93b55fcb10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEvMyK5Nggu7",
        "outputId": "a2d2b6bb-96f5-4764-aa3c-b1d93094740a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
        "tgt_input = tensor[:, 1:]    # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "453mO9tbgjga",
        "outputId": "112ad928-d900-4353-8013-eb02d1b4b858"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0]\n",
            "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=2020)"
      ],
      "metadata": {
        "id": "Tc3lNN5EkrXK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1    #tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTNUSoKPku8h",
        "outputId": "91e3b4e8-ab07-49c5-dd86-ede1978bf36e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV_oDVizk-7L",
        "outputId": "8d2823c5-3cca-4a1e-cf9b-996c857d09d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T12oqTVBlAlV",
        "outputId": "5dff6876-a284-46a0-e550-84a7a8f1a51f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Train: (140599, 14)\n",
            "Target Train: (140599, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. 인공지능 만들기"
      ],
      "metadata": {
        "id": "HtTJzwndlDb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 1024\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "lGTgJ7FglDyx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2AsN2Ewl8az",
        "outputId": "3ca5c80b-9b3d-434e-a68b-58deac48c96e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-3.3148783e-04, -3.0133448e-04,  2.9184605e-04, ...,\n",
              "          9.4496791e-04, -3.3875514e-04,  3.3647809e-04],\n",
              "        [-4.9975258e-04, -3.9914821e-04,  2.7321753e-04, ...,\n",
              "          1.4814349e-03, -4.4690745e-04,  8.2452170e-04],\n",
              "        [-2.7016178e-04, -2.9424927e-04, -2.8974027e-04, ...,\n",
              "          1.3009158e-03, -1.0405949e-03,  6.1747810e-04],\n",
              "        ...,\n",
              "        [ 5.0513945e-03,  4.3851337e-03, -2.6042729e-03, ...,\n",
              "          1.3314621e-03, -4.0189400e-03, -8.9151837e-04],\n",
              "        [ 6.4404518e-03,  4.5906277e-03, -3.2656319e-03, ...,\n",
              "          1.3593787e-03, -4.4834777e-03, -1.3136821e-03],\n",
              "        [ 7.6870555e-03,  4.7297752e-03, -3.9017557e-03, ...,\n",
              "          1.4002451e-03, -4.9147834e-03, -1.7100894e-03]],\n",
              "\n",
              "       [[-3.3148783e-04, -3.0133448e-04,  2.9184605e-04, ...,\n",
              "          9.4496791e-04, -3.3875514e-04,  3.3647809e-04],\n",
              "        [-3.5684885e-04, -6.6404667e-04,  6.5607060e-04, ...,\n",
              "          1.2696256e-03,  7.6052794e-07,  3.1338737e-04],\n",
              "        [-4.5365500e-04, -1.2560610e-03,  8.8119105e-04, ...,\n",
              "          9.0993382e-04,  2.1714058e-04,  2.1617857e-04],\n",
              "        ...,\n",
              "        [ 2.4774936e-03,  1.5748008e-03, -4.3588763e-04, ...,\n",
              "          1.6555703e-03, -1.2015087e-03, -1.4769022e-03],\n",
              "        [ 4.0130015e-03,  1.9678818e-03, -1.0544854e-03, ...,\n",
              "          1.5391888e-03, -1.8597345e-03, -1.8807374e-03],\n",
              "        [ 5.4671750e-03,  2.3165860e-03, -1.7189559e-03, ...,\n",
              "          1.4826106e-03, -2.5208835e-03, -2.2378319e-03]],\n",
              "\n",
              "       [[-3.3148783e-04, -3.0133448e-04,  2.9184605e-04, ...,\n",
              "          9.4496791e-04, -3.3875514e-04,  3.3647809e-04],\n",
              "        [-1.0904458e-03, -3.2811603e-04,  1.9575574e-04, ...,\n",
              "          1.8163099e-03, -7.0112239e-04,  3.8355708e-04],\n",
              "        [-1.6343243e-03, -6.7690562e-04,  6.7163934e-04, ...,\n",
              "          2.1196967e-03, -1.2584854e-03,  4.6861626e-04],\n",
              "        ...,\n",
              "        [ 5.0830864e-03,  2.4235905e-03, -3.2295517e-03, ...,\n",
              "          6.9788966e-04, -3.1836303e-03, -8.2206924e-04],\n",
              "        [ 6.5384959e-03,  2.7990534e-03, -3.8833339e-03, ...,\n",
              "          8.0516393e-04, -3.7365912e-03, -1.2629123e-03],\n",
              "        [ 7.7949627e-03,  3.1129427e-03, -4.4920677e-03, ...,\n",
              "          9.5286005e-04, -4.2608362e-03, -1.6755889e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-3.3148783e-04, -3.0133448e-04,  2.9184605e-04, ...,\n",
              "          9.4496791e-04, -3.3875514e-04,  3.3647809e-04],\n",
              "        [-7.5659482e-04, -3.0308624e-04,  5.0325360e-04, ...,\n",
              "          1.8015894e-03, -2.6086075e-04,  4.8724021e-04],\n",
              "        [-8.9889608e-04, -1.5046759e-04,  5.0398073e-04, ...,\n",
              "          2.0338683e-03, -3.3875945e-04,  7.7760931e-05],\n",
              "        ...,\n",
              "        [-6.2886940e-04,  4.1531148e-05, -1.8182776e-03, ...,\n",
              "          1.1017623e-03, -1.4097668e-03, -5.2060455e-04],\n",
              "        [ 6.6828093e-04,  7.6292857e-04, -2.2253068e-03, ...,\n",
              "          7.5284089e-04, -2.0581244e-03, -8.3787116e-04],\n",
              "        [ 2.2069453e-03,  1.3971074e-03, -2.7070032e-03, ...,\n",
              "          5.2372605e-04, -2.7024373e-03, -1.1951479e-03]],\n",
              "\n",
              "       [[-3.4484392e-04, -2.3011660e-04,  1.6866713e-04, ...,\n",
              "         -2.1737565e-04,  3.4115330e-04,  6.1352890e-05],\n",
              "        [-8.1114244e-04, -3.0856032e-04,  6.8141948e-05, ...,\n",
              "         -5.1177590e-04,  1.3844903e-04,  6.2725629e-04],\n",
              "        [-1.2326838e-03, -5.2246294e-04, -3.6103922e-04, ...,\n",
              "         -9.5567433e-04, -3.1580352e-05,  9.0297317e-04],\n",
              "        ...,\n",
              "        [-1.4683019e-03, -8.1116991e-04,  1.7783323e-03, ...,\n",
              "          1.3000992e-05, -4.3335651e-05,  1.8376024e-03],\n",
              "        [-1.3908776e-03, -7.6822611e-04,  1.8114718e-03, ...,\n",
              "          2.7969378e-05, -4.0545780e-04,  2.0070979e-03],\n",
              "        [-1.3293575e-03, -6.3480076e-04,  1.2137142e-03, ...,\n",
              "         -4.0132899e-04, -1.4515840e-04,  2.0977217e-03]],\n",
              "\n",
              "       [[-3.3148783e-04, -3.0133448e-04,  2.9184605e-04, ...,\n",
              "          9.4496791e-04, -3.3875514e-04,  3.3647809e-04],\n",
              "        [-1.0904458e-03, -3.2811603e-04,  1.9575574e-04, ...,\n",
              "          1.8163099e-03, -7.0112239e-04,  3.8355708e-04],\n",
              "        [-1.6315955e-03, -5.4699962e-04,  4.3698830e-05, ...,\n",
              "          2.8100270e-03, -1.0503196e-03,  3.5142855e-04],\n",
              "        ...,\n",
              "        [-2.4975566e-04,  1.7968781e-03, -1.9242100e-03, ...,\n",
              "          2.8547260e-03, -9.8177232e-04,  1.8416347e-04],\n",
              "        [ 5.6772528e-04,  2.3749222e-03, -2.0938946e-03, ...,\n",
              "          2.3691971e-03, -1.4670747e-03,  1.6365442e-04],\n",
              "        [ 1.8347880e-03,  2.7287134e-03, -2.4339044e-03, ...,\n",
              "          1.9326468e-03, -2.0659466e-03, -2.4981844e-05]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
        "    from_logits=True, reduction='none') # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True."
      ],
      "metadata": {
        "id": "8gFFXuyJmPU5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B55KIdlmm8c",
        "outputId": "ad1280c3-27ae-4193-93d1-85f5a9edfd7b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  12289024  \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  25174016  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  33562624  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  24590049  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95,615,713\n",
            "Trainable params: 95,615,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qiye68bnBaj",
        "outputId": "fb9f0314-ed63-4da2-d21d-4dce1ae02543"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "549/549 [==============================] - 257s 464ms/step - loss: 3.2951\n",
            "Epoch 2/7\n",
            "549/549 [==============================] - 270s 492ms/step - loss: 2.7148\n",
            "Epoch 3/7\n",
            "549/549 [==============================] - 267s 487ms/step - loss: 2.3068\n",
            "Epoch 4/7\n",
            "549/549 [==============================] - 271s 494ms/step - loss: 1.9365\n",
            "Epoch 5/7\n",
            "549/549 [==============================] - 271s 494ms/step - loss: 1.6161\n",
            "Epoch 6/7\n",
            "549/549 [==============================] - 268s 488ms/step - loss: 1.3570\n",
            "Epoch 7/7\n",
            "549/549 [==============================] - 271s 494ms/step - loss: 1.1628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93b31d4ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
        "\n",
        "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
        "        test_tensor = tf.concat([test_tensor, \n",
        "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
      ],
      "metadata": {
        "id": "X6nLXO9KMYSI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jldVy_86Mbek",
        "outputId": "894ab238-6c62-45db-ac83-7e5734244ac4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e2jH6N96KkSj",
        "outputId": "1a3cb0e9-6d66-4185-cbce-288547661513"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> baby , baby , baby <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> cause i want\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-8QgXfJNKm9E",
        "outputId": "a032b49a-b7eb-4e4f-bf01-1a95ab53437e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> cause i want to be on park pushin a benz <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i can see\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0UdYuxuWKq8Z",
        "outputId": "244bcb54-5f62-4f1e-f5c0-eb48159ffab5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i can see it through my chest <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정리\n",
        "\n",
        "Step 1. 데이터 다운로드\n",
        "\n",
        "- 각 파일을 읽은 후, 파일의 문장단위를 저장해 말뭉치(courpus)를 만든다.\n",
        "\n",
        "Step 2. 데이터 읽어오기\n",
        "\n",
        "- glob모듈을 이용해 파일을 쉽게 읽을 수 있다.\n",
        "\n",
        "Step 3. 데이터 정제\n",
        "\n",
        "- 영문을 모두 소문자로 변환한다.\n",
        "- 특수문자, 공백 패턴 등을 모두 제거한다.\n",
        "- 단어에 공백을 주어 토큰화가 쉽게해준다.\n",
        "- 토크나이저 객체를 생성한다.\n",
        "- 토크나이저를 통해 사전을 구축하고, 사전에서 tensor로 변환한다.\n",
        "\n",
        "Step 4. 평가 데이터셋 분리\n",
        "\n",
        "- tf.data.Dataset을 통해 학습 데이터셋, 평가 데이터셋을 만든다.\n",
        "\n",
        "\n",
        "Step 5. 인공지능 만들기\n",
        "\n",
        "- 모델을 만들어 학습을 진행한다."
      ],
      "metadata": {
        "id": "3se2b56I3nOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 루브릭\n",
        "1. 데이터의 전처리 및 구성과정이 체계적으로 진행되었는가?\n",
        "\n",
        " 데이터 다운로드 -> 읽어오기 -> 정제 -> 분리 과정등을 통해 진행되었다.\n",
        "\n",
        "2. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
        "\n",
        " 잘 생성되었다.\n",
        "\n",
        "3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
        " \n",
        " embedding size=1024, hidden size=2048로 설정해 epoch 7회를 진행한 결과, validation loss 1.1628 를 달성했습니다."
      ],
      "metadata": {
        "id": "zwW5tFAZ5uqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7bgoGJjw5RAE"
      }
    }
  ]
}